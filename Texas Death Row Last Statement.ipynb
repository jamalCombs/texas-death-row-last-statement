{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import Counter\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tally_occurrences(df_list):\n",
    "    counter = Counter()\n",
    "    \n",
    "    for word in df_list:\n",
    "        counter[word] += 1\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def last_statement_counter(filepath):\n",
    "    counter = Counter()\n",
    "    \n",
    "    df = pd.read_csv(filepath, encoding=\"iso-8859-1\")\n",
    "    df[\"word_list\"] = df[\"LastStatement\"].str.replace(r\"[,\\/.:\\\"?]\", \"\").apply(lambda x: x.split())\n",
    "    # df[\"word_list\"] = df[\"word_list\"].apply(lambda x: x.split())\n",
    "    df[\"word_counter\"] = df[\"word_list\"].apply(lambda x: tally_occurrences(x))\n",
    "    df[\"most_common_words\"] = df[\"word_list\"].apply(lambda x: Counter(x).most_common(10))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Execution</th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>TDCJNumber</th>\n",
       "      <th>Age</th>\n",
       "      <th>Race</th>\n",
       "      <th>CountyOfConviction</th>\n",
       "      <th>AgeWhenReceived</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>NativeCounty</th>\n",
       "      <th>...</th>\n",
       "      <th>WhiteVictim</th>\n",
       "      <th>HispanicVictim</th>\n",
       "      <th>BlackVictim</th>\n",
       "      <th>VictimOther Races</th>\n",
       "      <th>FemaleVictim</th>\n",
       "      <th>MaleVictim</th>\n",
       "      <th>LastStatement</th>\n",
       "      <th>word_list</th>\n",
       "      <th>word_counter</th>\n",
       "      <th>most_common_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>545</td>\n",
       "      <td>Cardenas</td>\n",
       "      <td>Ruben</td>\n",
       "      <td>999275</td>\n",
       "      <td>47</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Hidalgo</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is my statement: My final words. First, I...</td>\n",
       "      <td>[This, is, my, statement, My, final, words, Fi...</td>\n",
       "      <td>{'This': 1, 'is': 1, 'my': 3, 'statement': 1, ...</td>\n",
       "      <td>[(I, 8), (for, 6), (to, 5), (me, 5), (and, 5),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>544</td>\n",
       "      <td>Pruett</td>\n",
       "      <td>Robert</td>\n",
       "      <td>999411</td>\n",
       "      <td>38</td>\n",
       "      <td>White</td>\n",
       "      <td>Bee</td>\n",
       "      <td>22.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I just want to let everyone in here to know I ...</td>\n",
       "      <td>[I, just, want, to, let, everyone, in, here, t...</td>\n",
       "      <td>{'I': 4, 'just': 1, 'want': 1, 'to': 5, 'let':...</td>\n",
       "      <td>[(to, 5), (I, 4), (love, 3), (so, 3), (much, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>543</td>\n",
       "      <td>Preyor</td>\n",
       "      <td>Taichin</td>\n",
       "      <td>999494</td>\n",
       "      <td>46</td>\n",
       "      <td>Black</td>\n",
       "      <td>Bexar</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>First and foremost I'd like to say, \"Justice h...</td>\n",
       "      <td>[First, and, foremost, I'd, like, to, say, Jus...</td>\n",
       "      <td>{'First': 1, 'and': 3, 'foremost': 1, 'I'd': 1...</td>\n",
       "      <td>[(and, 3), (to, 3), (by, 2), (my, 2), (First, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>542</td>\n",
       "      <td>Bigby</td>\n",
       "      <td>James</td>\n",
       "      <td>997</td>\n",
       "      <td>61</td>\n",
       "      <td>White</td>\n",
       "      <td>Tarrant</td>\n",
       "      <td>36.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes, I do, Grace Kehler is that you? I have gi...</td>\n",
       "      <td>[Yes, I, do, Grace, Kehler, is, that, you, I, ...</td>\n",
       "      <td>{'Yes': 1, 'I': 12, 'do': 1, 'Grace': 2, 'Kehl...</td>\n",
       "      <td>[(I, 12), (you, 10), (I'm, 8), (sorry, 8), (th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>541</td>\n",
       "      <td>Ruiz</td>\n",
       "      <td>Rolando</td>\n",
       "      <td>999145</td>\n",
       "      <td>44</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Bexar</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes sir, I would first like to say to the Sanc...</td>\n",
       "      <td>[Yes, sir, I, would, first, like, to, say, to,...</td>\n",
       "      <td>{'Yes': 1, 'sir': 1, 'I': 7, 'would': 1, 'firs...</td>\n",
       "      <td>[(I, 7), (you, 5), (and, 4), (to, 3), (am, 3),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>5</td>\n",
       "      <td>Skillern</td>\n",
       "      <td>Doyle</td>\n",
       "      <td>518</td>\n",
       "      <td>49</td>\n",
       "      <td>White</td>\n",
       "      <td>Lubbock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I pray that my family will rejoice and will fo...</td>\n",
       "      <td>[I, pray, that, my, family, will, rejoice, and...</td>\n",
       "      <td>{'I': 1, 'pray': 1, 'that': 1, 'my': 1, 'famil...</td>\n",
       "      <td>[(will, 2), (I, 1), (pray, 1), (that, 1), (my,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>4</td>\n",
       "      <td>Barefoot</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>621</td>\n",
       "      <td>39</td>\n",
       "      <td>White</td>\n",
       "      <td>Bell</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>When asked if he had a last statement, he repl...</td>\n",
       "      <td>[When, asked, if, he, had, a, last, statement,...</td>\n",
       "      <td>{'When': 1, 'asked': 1, 'if': 1, 'he': 3, 'had...</td>\n",
       "      <td>[(I, 6), (to, 5), (that, 4), (the, 4), (he, 3)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Bryan</td>\n",
       "      <td>Ronald</td>\n",
       "      <td>529</td>\n",
       "      <td>39</td>\n",
       "      <td>White</td>\n",
       "      <td>Harris</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What is about to transpire in a few moments is...</td>\n",
       "      <td>[What, is, about, to, transpire, in, a, few, m...</td>\n",
       "      <td>{'What': 1, 'is': 4, 'about': 1, 'to': 3, 'tra...</td>\n",
       "      <td>[(I, 8), (in, 6), (and, 5), (all, 5), (my, 5),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>2</td>\n",
       "      <td>Autry</td>\n",
       "      <td>James</td>\n",
       "      <td>670</td>\n",
       "      <td>29</td>\n",
       "      <td>White</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>[None]</td>\n",
       "      <td>{'None': 1}</td>\n",
       "      <td>[(None, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>1</td>\n",
       "      <td>Brooks, Jr.</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>592</td>\n",
       "      <td>40</td>\n",
       "      <td>Black</td>\n",
       "      <td>Tarrant</td>\n",
       "      <td>35.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Statement to the Media: I, at this very moment...</td>\n",
       "      <td>[Statement, to, the, Media, I, at, this, very,...</td>\n",
       "      <td>{'Statement': 1, 'to': 6, 'the': 5, 'Media': 1...</td>\n",
       "      <td>[(Allah, 9), (I, 8), (to, 6), (the, 5), (is, 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>545 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Execution     LastName FirstName  TDCJNumber  Age      Race  \\\n",
       "0          545     Cardenas     Ruben      999275   47  Hispanic   \n",
       "1          544       Pruett    Robert      999411   38     White   \n",
       "2          543       Preyor   Taichin      999494   46     Black   \n",
       "3          542        Bigby     James         997   61     White   \n",
       "4          541         Ruiz   Rolando      999145   44  Hispanic   \n",
       "..         ...          ...       ...         ...  ...       ...   \n",
       "540          5     Skillern     Doyle         518   49     White   \n",
       "541          4     Barefoot    Thomas         621   39     White   \n",
       "542          3      O'Bryan    Ronald         529   39     White   \n",
       "543          2        Autry     James         670   29     White   \n",
       "544          1  Brooks, Jr.   Charlie         592   40     Black   \n",
       "\n",
       "    CountyOfConviction  AgeWhenReceived  EducationLevel  NativeCounty   ...  \\\n",
       "0              Hidalgo             28.0            11.0            1.0  ...   \n",
       "1                  Bee             22.0             8.0            0.0  ...   \n",
       "2                Bexar             34.0            10.0            0.0  ...   \n",
       "3              Tarrant             36.0             9.0            0.0  ...   \n",
       "4                Bexar             22.0            10.0            0.0  ...   \n",
       "..                 ...              ...             ...            ...  ...   \n",
       "540            Lubbock              NaN             NaN            NaN  ...   \n",
       "541               Bell             33.0             NaN            NaN  ...   \n",
       "542             Harris             31.0             NaN            NaN  ...   \n",
       "543          Jefferson             26.0             6.0            0.0  ...   \n",
       "544            Tarrant             35.0            12.0            0.0  ...   \n",
       "\n",
       "     WhiteVictim  HispanicVictim  BlackVictim  VictimOther Races  \\\n",
       "0            0.0             1.0          0.0                0.0   \n",
       "1            1.0             0.0          0.0                0.0   \n",
       "2            1.0             0.0          0.0                0.0   \n",
       "3            1.0             0.0          0.0                0.0   \n",
       "4            0.0             1.0          0.0                0.0   \n",
       "..           ...             ...          ...                ...   \n",
       "540          NaN             NaN          NaN                NaN   \n",
       "541          NaN             NaN          NaN                NaN   \n",
       "542          1.0             0.0          0.0                0.0   \n",
       "543          NaN             NaN          NaN                NaN   \n",
       "544          1.0             0.0          0.0                0.0   \n",
       "\n",
       "     FemaleVictim  MaleVictim  \\\n",
       "0             1.0         0.0   \n",
       "1             0.0         1.0   \n",
       "2             0.0         1.0   \n",
       "3             0.0         1.0   \n",
       "4             1.0         0.0   \n",
       "..            ...         ...   \n",
       "540           0.0         1.0   \n",
       "541           1.0         1.0   \n",
       "542           0.0         1.0   \n",
       "543           1.0         0.0   \n",
       "544           0.0         1.0   \n",
       "\n",
       "                                         LastStatement  \\\n",
       "0    This is my statement: My final words. First, I...   \n",
       "1    I just want to let everyone in here to know I ...   \n",
       "2    First and foremost I'd like to say, \"Justice h...   \n",
       "3    Yes, I do, Grace Kehler is that you? I have gi...   \n",
       "4    Yes sir, I would first like to say to the Sanc...   \n",
       "..                                                 ...   \n",
       "540  I pray that my family will rejoice and will fo...   \n",
       "541  When asked if he had a last statement, he repl...   \n",
       "542  What is about to transpire in a few moments is...   \n",
       "543                                               None   \n",
       "544  Statement to the Media: I, at this very moment...   \n",
       "\n",
       "                                             word_list  \\\n",
       "0    [This, is, my, statement, My, final, words, Fi...   \n",
       "1    [I, just, want, to, let, everyone, in, here, t...   \n",
       "2    [First, and, foremost, I'd, like, to, say, Jus...   \n",
       "3    [Yes, I, do, Grace, Kehler, is, that, you, I, ...   \n",
       "4    [Yes, sir, I, would, first, like, to, say, to,...   \n",
       "..                                                 ...   \n",
       "540  [I, pray, that, my, family, will, rejoice, and...   \n",
       "541  [When, asked, if, he, had, a, last, statement,...   \n",
       "542  [What, is, about, to, transpire, in, a, few, m...   \n",
       "543                                             [None]   \n",
       "544  [Statement, to, the, Media, I, at, this, very,...   \n",
       "\n",
       "                                          word_counter  \\\n",
       "0    {'This': 1, 'is': 1, 'my': 3, 'statement': 1, ...   \n",
       "1    {'I': 4, 'just': 1, 'want': 1, 'to': 5, 'let':...   \n",
       "2    {'First': 1, 'and': 3, 'foremost': 1, 'I'd': 1...   \n",
       "3    {'Yes': 1, 'I': 12, 'do': 1, 'Grace': 2, 'Kehl...   \n",
       "4    {'Yes': 1, 'sir': 1, 'I': 7, 'would': 1, 'firs...   \n",
       "..                                                 ...   \n",
       "540  {'I': 1, 'pray': 1, 'that': 1, 'my': 1, 'famil...   \n",
       "541  {'When': 1, 'asked': 1, 'if': 1, 'he': 3, 'had...   \n",
       "542  {'What': 1, 'is': 4, 'about': 1, 'to': 3, 'tra...   \n",
       "543                                        {'None': 1}   \n",
       "544  {'Statement': 1, 'to': 6, 'the': 5, 'Media': 1...   \n",
       "\n",
       "                                     most_common_words  \n",
       "0    [(I, 8), (for, 6), (to, 5), (me, 5), (and, 5),...  \n",
       "1    [(to, 5), (I, 4), (love, 3), (so, 3), (much, 3...  \n",
       "2    [(and, 3), (to, 3), (by, 2), (my, 2), (First, ...  \n",
       "3    [(I, 12), (you, 10), (I'm, 8), (sorry, 8), (th...  \n",
       "4    [(I, 7), (you, 5), (and, 4), (to, 3), (am, 3),...  \n",
       "..                                                 ...  \n",
       "540  [(will, 2), (I, 1), (pray, 1), (that, 1), (my,...  \n",
       "541  [(I, 6), (to, 5), (that, 4), (the, 4), (he, 3)...  \n",
       "542  [(I, 8), (in, 6), (and, 5), (all, 5), (my, 5),...  \n",
       "543                                        [(None, 1)]  \n",
       "544  [(Allah, 9), (I, 8), (to, 6), (the, 5), (is, 5...  \n",
       "\n",
       "[545 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texas_last_df = last_statement_counter(\"./Texas Last Statement.csv\"); texas_last_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_of_column_names(string, column_number_list):\n",
    "    counter = 0\n",
    "    column_name_list = []\n",
    "    \n",
    "    for item in column_number_list:\n",
    "        counter+=1\n",
    "        column_name_list.append(\"{}_{}\".format(string, counter))\n",
    "    \n",
    "    return column_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_aggregation(word_df):\n",
    "    word_df_list = word_df.values.tolist()\n",
    "    word_df_aggregation = []\n",
    "\n",
    "    for item in word_df_list:\n",
    "        for word in item:\n",
    "            if type(word) != float and word != \"None\":\n",
    "                word_df_aggregation.append(word)\n",
    "    \n",
    "    return word_df_aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Tests\n",
    "\n",
    "a_list_one = [(\"I\", 8), (\"for\", 6), (\"to\", 5), (\"me\", 5), (\"and\", 5)]\n",
    "words, occurrences = [list(item) for item in zip(*a_list_one)]; words\n",
    "updated_df = pd.DataFrame(texas_last_df[\"most_common_words\"].str.split().values.tolist()); updated_df\n",
    "temp_df_one = pd.DataFrame.from_records(a_list_one, columns=[\"word\", \"occurrence\"]); temp_df_one\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOP NUMBER OF VICTIMS (subset)\n",
    "Which person has the highest number of victims?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_number_of_victim = texas_last_df[texas_last_df[\"NumberVictim\"] >= 3]; top_number_of_victim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_unzip = top_number_of_victim[\"most_common_words\"].apply(lambda x: [list(item) for item in zip(*x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_df = pd.DataFrame(occurrences_unzip.apply(pd.Series))\n",
    "occurrences_df.columns = [\"word\", \"occurrences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = occurrences_df[\"word\"].apply(pd.Series)\n",
    "words_df.columns = create_list_of_column_names(\"word\", words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_number_df = occurrences_df[\"occurrences\"].apply(pd.Series).fillna(0).apply(lambda x: x.astype(int))\n",
    "occurrences_number_df.columns = create_list_of_column_names(\"occurrences_number\", occurrences_number_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_word_occurrences_df = pd.concat([words_df, occurrences_number_df], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOP VICTIMS WORD OCCURRENCES AGGREGATION\n",
    "Parse rows to find most common words used overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_aggregation = get_word_aggregation(words_df)\n",
    "top_word_counter = Counter(top_word_aggregation).most_common(20)\n",
    "offenders_with_five_victims = pd.DataFrame.from_records(top_word_counter, columns=[\"word\", \"occurrence\"]); offenders_with_five_victims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL OFFENDERS\n",
    "Checkout all offenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_offenders = texas_last_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_unzip = all_offenders[\"most_common_words\"].apply(lambda x: [list(item) for item in zip(*x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_occurrences_df = pd.DataFrame(occurrences_unzip.apply(pd.Series))\n",
    "all_occurrences_df.columns = [\"word\", \"occurrences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_df = all_occurrences_df[\"word\"].apply(pd.Series)\n",
    "all_words_df.columns = create_list_of_column_names(\"word\", all_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurrence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>you</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>the</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>my</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>for</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>love</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>all</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>that</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>of</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>me</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>am</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>have</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>a</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>family</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>would</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>is</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>like</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>in</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  occurrence\n",
       "0        I         393\n",
       "1       to         292\n",
       "2      you         236\n",
       "3      and         221\n",
       "4      the         189\n",
       "5       my         147\n",
       "6      for         131\n",
       "7     love         123\n",
       "8      all         106\n",
       "9     that          98\n",
       "10      of          94\n",
       "11      me          90\n",
       "12      am          75\n",
       "13    have          62\n",
       "14       a          56\n",
       "15  family          53\n",
       "16   would          47\n",
       "17      is          47\n",
       "18    like          43\n",
       "19      in          42"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_word_aggregation = get_word_aggregation(all_words_df)\n",
    "all_word_counter = Counter(all_word_aggregation).most_common(20)\n",
    "all_offenders_aggregation_df = pd.DataFrame.from_records(all_word_counter, columns=[\"word\", \"occurrence\"]); all_offenders_aggregation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL TOP VICTIMS DATAFRAMES\n",
    "Concatenate Top Victim and word occurrence collection into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_occurrences_df = pd.concat([top_number_of_victim, combined_word_occurrences_df,], axis=1, sort=False); top_occurrences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_last_cleaned_df.to_csv(\"texas_last_statement_cleaned.csv \", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_offenders_aggregation_df.to_csv(\"all_offenders.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
